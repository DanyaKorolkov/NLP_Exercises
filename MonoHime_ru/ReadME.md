**Загрузка данных**

Так как файлы большие, то прикреплять сюда я их не буду. В ходе работы я подгружал уже обработанные данные с Google Drive.

Загрузить данные можно напрямую с HuggingFace с помощью команды:

```python
from datasets import load_dataset

data_path = "MonoHime/ru_sentiment_dataset"
dataset = load_dataset(data_path, revision="main")
```
**Сравнение моделей**

В ходе работы сравнивалась работа бустинга на различных эмбеддингах, а именно на:
* TF-IDF Vectorizer
* BERT-TINY
* LASER

В качестве бустинга брался LGBMClassifier без подбора гиперпараметров (с дефолтными при пустой инициализации)

Алгоритмам подавался как обработанный текст (text_prep), так и чистый необработанный (text)

| Метрика \ Векторизация | TF-IDF (text_prep) | BERT (TINY-512, text) | BERT (TINY-512, text_prep) | LASER (1024, text_prep) |  LASER (1024, text) |
|------------------------|--------------------|-----------------------|----------------------------|-------------------------|---------------------|
| Precision Macro        |       0.70         |        0.57           |           0.54             |           0.70          |      **0.74**       |
| Recall Macro           |       0.69         |        0.53           |           0.50             |           0.69          |      **0.73**       |
| Accuracy               |       0.71         |        0.54           |           0.56             |           0.71          |      **0.75**       |
| Roc_Auc Macro          |       0.88         |        0.765          |           0.740            |           0.882         |      **0.905**      |

* TF-IDF (text) незначительно хуже в каждой из метрик.

**Ускорение CPU -> GPU (2xT4)**

*Получение эмбеддингов.*
В сравнении брался один и тот же батч размера 128 векторов.

|  Векторизация \ Device  |      CPU     |  GPU     |  Diff
|-------------------------|--------------|----------| ------|
|        BERT-Distil      | 2 мин 18 сек |  2 сек   |  x69  |
|        BERT-Tiny        |    3.6 сек   | 0.24 сек |  x15  |
|        LASER            |    36 сек    | 1.3 сек  |  x27  |

*Обучение и инференс классификатора.*
Брался LGBMClassifier на 500 деревьев.
Задача многоклассовой классификации - под капотом Кросс-Энтропия в качестве функции потерь.

Метод \ Device|     CPU     |    GPU   |
|-------------|-------------|----------|
|     FIT     | 4 мин 3 сек |  42 сек  |
|   PREDICT   |   3.7 сек   | 3.54 сек |
